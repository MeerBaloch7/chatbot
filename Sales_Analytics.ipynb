{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sales Data Analytics Chatbot\n",
        "i have build a chatbot using `Langchain` and `HuggingFace`\n",
        "\n",
        "Model used: **`Llama 2-7B`**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DiyFpKoJOhR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## libraries to be installed"
      ],
      "metadata": {
        "id": "qT5dECLpPYsp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj0pt3_XOXf5"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit-chat\n",
        "!pip install -U langchain-community\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-gpu\n",
        "!pip install ctransformers\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model from huggingface\n",
        "[model](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf)"
      ],
      "metadata": {
        "id": "4DxTuj-bPuE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\"\n",
        "wget.download(model_url)"
      ],
      "metadata": {
        "id": "NyI29b4KP858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## writing all code in single python file"
      ],
      "metadata": {
        "id": "r4fO32MZQVwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app2.py\n",
        "import streamlit as st\n",
        "from streamlit_chat import message\n",
        "import tempfile\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
        "\n",
        "#Loading the model\n",
        "def load_llm():\n",
        "    # Load the locally downloaded model here\n",
        "    llm = CTransformers(\n",
        "        model = \"/content/llama-2-7b-chat.Q2_K.gguf\",\n",
        "        model_type=\"llama\",\n",
        "        max_new_tokens = 512,\n",
        "        temperature = 0.5\n",
        "    )\n",
        "    return llm\n",
        "\n",
        "\n",
        "st.title(\"Sales data analytics with Llama 2 ðŸ¦™ðŸ¦œ\")\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload your Data\", type=\"csv\")\n",
        "\n",
        "if uploaded_file :\n",
        "   #use tempfile because CSVLoader only accepts a file_path\n",
        "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    loader = CSVLoader(file_path=tmp_file_path, encoding=\"utf-8\", csv_args={\n",
        "                'delimiter': ','})\n",
        "    data = loader.load()\n",
        "    #st.json(data)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "\n",
        "    db = FAISS.from_documents(data, embeddings)\n",
        "    db.save_local(DB_FAISS_PATH)\n",
        "    llm = load_llm()\n",
        "    chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever(search_kwargs={'k': 2}),return_source_documents=True)\n",
        "\n",
        "    def conversational_chat(query):\n",
        "        result = chain({\"question\": query, \"chat_history\": st.session_state['history']})\n",
        "        st.session_state['history'].append((query, result[\"answer\"]))\n",
        "        return result[\"answer\"]\n",
        "\n",
        "    if 'history' not in st.session_state:\n",
        "        st.session_state['history'] = []\n",
        "\n",
        "    if 'generated' not in st.session_state:\n",
        "        st.session_state['generated'] = [\"Hello ! Ask me anything about \" + uploaded_file.name + \" ðŸ¤—\"]\n",
        "\n",
        "    if 'past' not in st.session_state:\n",
        "        st.session_state['past'] = [\"Hey ! ðŸ‘‹\"]\n",
        "\n",
        "    #container for the chat history\n",
        "    response_container = st.container()\n",
        "    #container for the user's text input\n",
        "    container = st.container()\n",
        "\n",
        "    with container:\n",
        "        with st.form(key='my_form', clear_on_submit=True):\n",
        "\n",
        "            user_input = st.text_input(\"Query:\", placeholder=\"Talk to your csv data here (:\", key='input')\n",
        "            submit_button = st.form_submit_button(label='Send')\n",
        "\n",
        "        if submit_button and user_input:\n",
        "            output = conversational_chat(user_input)\n",
        "\n",
        "            st.session_state['past'].append(user_input)\n",
        "            st.session_state['generated'].append(output)\n",
        "\n",
        "    if st.session_state['generated']:\n",
        "        with response_container:\n",
        "            for i in range(len(st.session_state['generated'])):\n",
        "                message(st.session_state[\"past\"][i], is_user=True, key=str(i) + '_user', avatar_style=\"big-smile\")\n",
        "                message(st.session_state[\"generated\"][i], key=str(i), avatar_style=\"thumbs\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UChNCpdjQTxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### install local tunnel"
      ],
      "metadata": {
        "id": "rvZPIBqgQeaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "kev8-WYCQoF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the streamlit App locally"
      ],
      "metadata": {
        "id": "o9EdxcRrQqDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app2.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "M5dNY1k6QpXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}